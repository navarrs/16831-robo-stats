{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3lRSKm9LZFQ"
   },
   "source": [
    "# 16831: Homework 5 - Behavior Cloning, DAGGER\n",
    "\n",
    "You will implement this assignment right here in this Jupyter notebook. ote that all cells modify the same global state, so imported packages as well as functions and variables declared in one cell will be accessible in other cells.\n",
    "\n",
    "You will want to run each cell in this notebook by clicking the \"Run' button in the tool bar on top of the notebook (or using [ctrl -> enter]. Look for ``WRITE CODE HERE'' to identify places where you need to write some code. Each section involves writing 3 - 10 lines of code. \n",
    "\n",
    "When you're done, copy plots genetated by your code into your Latex writeup. Submite the notebook file in your code submission to Gradescope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDrq4jSWWYx1"
   },
   "source": [
    "# Preliminaries\n",
    "In these first few cells, you will implement some compoments that will be used for all problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIXzVYlFLL-m"
   },
   "source": [
    "\n",
    "### Setup: Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "MgHGKD-_iB3r",
    "outputId": "5ad0f247-936f-45f0-f0a1-b88cdcaa788c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "DirectoryNotACondaEnvironmentError: The target directory exists, but it is not a conda environment.\r\n",
      "Use 'conda create' to convert the directory to a conda environment.\r\n",
      "  target directory: /usr\r\n",
      "\r\n",
      "\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cb7bc0a1b717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict \n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YiASXZnSH7C"
   },
   "source": [
    "### Make the policy model\n",
    "We'll use the same architecture for each of the problems. By implementing a function that creates the model here, you won't need to implement it again for each problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cr7A-CqASErb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, nS, nA):\n",
    "        super(Policy, self).__init__()\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        \n",
    "        # WRITE CODE HERE\n",
    "        # Add layers to the model:\n",
    "        # a fully connected layer with 10 units\n",
    "        self.fc = nn.Linear(10)\n",
    "        # a tanh activation\n",
    "        # another fully connected layer with 2 units (the number of actions)\n",
    "        # a softmax activation (so the output is a proper distribution)\n",
    "        # We expect the model to have four weight variables (a kernel and bias for\n",
    "        # both layers)\n",
    "        assert len(list(self.model.parameters())) == 4, 'Model should have 4 weights.'\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "    def predict(self, state):\n",
    "        pred = self.model(torch.FloatTensor(state).to(device)).detach().cpu().numpy()\n",
    "        return pred\n",
    "    \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            pred = self.predict(state)\n",
    "            action = np.argmax(pred, axis=1)\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0z4oMxZSgq6"
   },
   "source": [
    "### Test the model\n",
    "To confirm that the model is correct, we'll use it to solve a binary classification problem. The target function $f: \\mathbb{R}^4 \\rightarrow {0, 1}$ indicates whether the sum of the vector coordinates is positive:\n",
    "$$f(x) = \\delta \\left(\\sum_{i=1}^4 x_i > 0 \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1iJByvzSp7_"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "nS, nA = 4, 2\n",
    "policy = Policy(nS, nA).to(device) # define policy\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "N = 2000\n",
    "X = np.random.normal(size=(N, nS))  # some random data\n",
    "is_positive = np.sum(X, axis=1) > 0  # A simple binary function\n",
    "Y = np.sum(X, axis=1) > 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "X = torch.FloatTensor(X).to(device)\n",
    "Y = torch.LongTensor(Y).to(device)\n",
    "batch_size = 256\n",
    "for epoch in range(100):\n",
    "    idxes = np.random.permutation(N)\n",
    "    losses = []\n",
    "    acces = []\n",
    "    for i in range(N // batch_size):\n",
    "        idx = idxes[i*batch_size: (i+1) * batch_size]\n",
    "        x, y = X[idx], Y[idx]\n",
    "        pred_y = policy(x)\n",
    "        loss = criterion(pred_y, y)\n",
    "        acc = torch.sum(torch.argmax(pred_y, dim=1) == y) /  y.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        acces.append(acc.item())\n",
    "    if epoch %10 ==0:\n",
    "        print('epoch {}, loss {:.3f}, accuracy {:.2f}'.format(epoch, np.mean(np.array(losses)), np.mean(np.array(acces))))\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw_HvyFnWXa5"
   },
   "source": [
    "### Interacting with the Gym\n",
    "Implement the function below for gathering an episode (a \"rollout\"). The environment we will use will implement the OpenAI Gym interface. For documentation, please see the link below:\n",
    "http://gym.openai.com/docs/#environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmRoHiliWdJf"
   },
   "outputs": [],
   "source": [
    "def action_to_one_hot(env, action):\n",
    "    action_vec = np.zeros(env.action_space.n)\n",
    "    action_vec[action] = 1\n",
    "    return action_vec    \n",
    "      \n",
    "      \n",
    "def generate_episode(env, policy):\n",
    "    \"\"\"Collects one rollout from the policy in an environment. The environment\n",
    "    should implement the OpenAI Gym interface. A rollout ends when done=True. The\n",
    "    number of states and actions should be the same, so you should not include\n",
    "    the final state when done=True.\n",
    "\n",
    "    Args:\n",
    "    env: an OpenAI Gym environment.\n",
    "    policy: a keras model\n",
    "    Returns:\n",
    "    states: a list of states visited by the agent.\n",
    "    actions: a list of actions taken by the agent. \n",
    "    rewards: the reward received by the agent at each step.\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        # WRITE CODE HERE\n",
    "    return np.array(states), np.array(actions), np.array(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9mCrbZDXVvI"
   },
   "source": [
    "### Test the data collection\n",
    "Run the following cell and make sure you see \"Test passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCo0B_aDXZfX"
   },
   "outputs": [],
   "source": [
    "# Create the environment.\n",
    "env = gym.make('CartPole-v0')\n",
    "nS = np.prod(env.observation_space.shape)\n",
    "nA = env.action_space.n\n",
    "\n",
    "policy = Policy(nS, nA).to(device)\n",
    "states, actions, rewards = generate_episode(env, policy)\n",
    "assert len(states) == len(actions), 'Number of states and actions should be equal.'\n",
    "assert len(actions) == len(rewards), 'Number of actions and rewards should be equal.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8mAIl5xLc6e"
   },
   "source": [
    "## Behavior Cloning and DAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYIziDr-VUG7"
   },
   "source": [
    "### Implementing Behavior Cloning and DAGGER\n",
    "To implement behavior cloning and DAGGER, fill in the missing blocks of code below. The provided code loads an expert model upon creation of the `Imitation` class. The function `generate_behavior_cloning_data()` fills in `self._train_states` and `self._train_actions` with states and actions from a single episode. Later, when implementing DAGGER, you will finish implementing `generate_dagger_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn2jQXBNh2WQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CartpoleExpertAgent():\n",
    "    def act(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            position, velocity, angle, angle_velocity = states\n",
    "            action = int(3. * angle + angle_velocity > 0.)\n",
    "        else:\n",
    "            position, velocity, angle, angle_velocity = states[:, 0], states[:, 1], states[:, 2], states[:, 3]\n",
    "            action = (3. * angle + angle_velocity > 0.).astype(np.int)\n",
    "        return action\n",
    "    \n",
    "class Imitation():\n",
    "\n",
    "    def __init__(self, env, num_timesteps):\n",
    "        self.env = env\n",
    "        self.expert = CartpoleExpertAgent()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.policy = Policy(nS, nA).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=1e-3)\n",
    "        \n",
    "    def generate_behavior_cloning_data(self):\n",
    "        self._train_states = []\n",
    "        self._train_actions = []\n",
    "        while len(self._train_states) < self.num_timesteps:\n",
    "            states, actions, rewards = generate_episode(self.env, self.expert)\n",
    "            self._train_states.extend(states)\n",
    "            self._train_actions.extend(actions)\n",
    "            \n",
    "        self._train_states = np.array(self._train_states)\n",
    "        self._train_actions = np.array(self._train_actions)\n",
    "        \n",
    "    def generate_dagger_data(self):\n",
    "        # WRITE CODE HERE\n",
    "        # You should collect states and actions from the student policy\n",
    "        # (self.policy), and then relabel the actions using the expert policy.\n",
    "        # This method does not return anything.\n",
    "        \n",
    "        \n",
    "    def train(self, num_epochs=200):\n",
    "        \"\"\"Trains the model on training data generated by the expert policy.\n",
    "        Args:\n",
    "          env: The environment to run the expert policy on.\n",
    "          num_epochs: number of epochs to train on the data generated by the expert.\n",
    "        Return:\n",
    "          loss: (float) final loss of the trained policy.\n",
    "          acc: (float) final accuracy of the trained policy\n",
    "        \"\"\"\n",
    "        # WRITE CODE HERE\n",
    "        X = self._train_states\n",
    "        Y = self._train_actions\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        X = torch.FloatTensor(X)\n",
    "        Y = torch.LongTensor(Y)\n",
    "        batch_size = 256\n",
    "        N = X.shape[0]\n",
    "        for epoch in range(num_epochs):\n",
    "            idxes = np.random.permutation(N)\n",
    "            losses = []\n",
    "            acces = []\n",
    "            for i in range(N // batch_size):\n",
    "                idx = idxes[i*batch_size: (i+1) * batch_size]\n",
    "                x, y = X[idx].to(device), Y[idx].to(device)\n",
    "                pred_y = self.policy(x)\n",
    "                loss = criterion(pred_y, y)\n",
    "                acc = torch.sum(torch.argmax(pred_y, dim=1) == y) /  y.shape[0]\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                acces.append(acc.item())\n",
    "        loss = losses[-1]\n",
    "        acc = acces[-1]\n",
    "        return loss, acc\n",
    "\n",
    "\n",
    "    def evaluate(self, policy, n_episodes=50):\n",
    "        rewards = []\n",
    "        for i in range(n_episodes):\n",
    "            _, _, r = generate_episode(self.env, policy)\n",
    "            rewards.append(sum(r))\n",
    "        r_mean = np.mean(rewards)\n",
    "        return r_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu7FcPOkAz-c"
   },
   "source": [
    "### Experiment: Student vs Expert\n",
    "In the next two cells, you will compare the performance of the expert policy\n",
    "to the imitation policies obtained via behavior cloning and DAGGER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "sRMPf6r2itw3",
    "outputId": "c80b426a-28dd-4f3a-c92a-3fde9e2362d2"
   },
   "outputs": [],
   "source": [
    "# Uncomment one of the two lines below to select whether to run behavior\n",
    "# cloning or dagger\n",
    "mode = 'behavior cloning'\n",
    "# mode = 'dagger'\n",
    "\n",
    "num_timesteps = 20000  # Leave this fixed for now. You will experiment with changing it later.\n",
    "num_iterations = 100  # Number of training iterations. Use a small number\n",
    "                     # (e.g., 10) for debugging, and then try a larger number\n",
    "                     # (e.g., 100).\n",
    "\n",
    "# Create the environment.\n",
    "env = gym.make('CartPole-v0')\n",
    "im = Imitation(env, num_timesteps)\n",
    "expert_reward = im.evaluate(im.expert)\n",
    "print('Expert reward: %.2f' % expert_reward)\n",
    "\n",
    "loss_vec = []\n",
    "acc_vec = []\n",
    "imitation_reward_vec = []\n",
    "for t in range(num_iterations):\n",
    "    if mode == 'behavior cloning':\n",
    "        im.generate_behavior_cloning_data()\n",
    "    elif mode == 'dagger':\n",
    "        im.generate_dagger_data()\n",
    "    else:\n",
    "        raise ValueError('Unknown mode: %s' % mode)\n",
    "    loss, acc = im.train(num_epochs=1)\n",
    "    imitation_reward = im.evaluate(im.policy)\n",
    "    loss_vec.append(loss)\n",
    "    acc_vec.append(acc)\n",
    "    imitation_reward_vec.append(imitation_reward)\n",
    "    print('(%d) loss = %.3f; accuracy = %.2f; reward = %.1f' % (t, loss, acc, imitation_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wacKZfLU1oAC"
   },
   "source": [
    "### Plot the results\n",
    "After saving your plots by running `plt.savefig(FILENAME)`, you can download them by navigating to the `Files` tab on the left, and then right-clicking on each filename and selecting `Download`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcytZUZYmrzc"
   },
   "outputs": [],
   "source": [
    "### Plot the results\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(131)\n",
    "plt.title('Reward')\n",
    "plt.plot(imitation_reward_vec, label='imitation')\n",
    "plt.hlines(expert_reward, 0, len(imitation_reward_vec), label='expert')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('return')\n",
    "plt.legend()\n",
    "plt.ylim([0, None])\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Loss')\n",
    "plt.plot(loss_vec)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(acc_vec)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.tight_layout()\n",
    "plt.savefig('student_vs_expert_%s.png' % mode, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4W6BRtNBGyv"
   },
   "source": [
    "### Experiment: How much expert data is needed?\n",
    "This question studies how the amount of expert data effects the performance. You will run the same experiment as above, each time varying the number of expert episodes collected at each iteration. Use values of 500, 1000, 5000, and 20000. You can keep the number of iterations fixed at 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sE8xQFW3ZbL"
   },
   "outputs": [],
   "source": [
    "random_seeds = 5\n",
    "# Dictionary mapping number of expert trajectories to a list of rewards.\n",
    "# Each is the result of running with a different random seed.\n",
    "all_timesteps = [500, 1000, 5000, 20000]\n",
    "num_iterations = 50\n",
    "reward_data, accuracy_data, loss_data = OrderedDict({}), OrderedDict({}), OrderedDict({})\n",
    "for num_timesteps in all_timesteps:\n",
    "    reward_data[num_timesteps] = []\n",
    "    accuracy_data[num_timesteps] = []\n",
    "    loss_data[num_timesteps] = []\n",
    "for num_timesteps in all_timesteps:\n",
    "    for t in range(random_seeds):\n",
    "        print('num_timesteps: %s; seed: %d' % (num_timesteps, t))\n",
    "        # WRITE CODE HERE\n",
    "        # Hint: The code here should be nearly identical to code after the\n",
    "        # \"Student vs Expert\" cell. Feel free to copy and paste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec4HEd163il1"
   },
   "source": [
    "Plot the reward, loss, and accuracy for each, remembering to label each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQzT0nPc5Odm"
   },
   "outputs": [],
   "source": [
    "keys = all_timesteps\n",
    "plt.figure(figsize=(16, 4))\n",
    "for (index, (data, name)) in enumerate(zip([reward_data, accuracy_data, loss_data],\n",
    "                                           ['reward', 'accuracy', 'loss'])):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    data = np.array(list(data.values())).reshape(len(all_timesteps), random_seeds, num_iterations)\n",
    "    for i, num_timesteps in enumerate(all_timesteps):\n",
    "        mean = np.mean(data[i, :, :], axis=0)\n",
    "        std = np.std(data[i, :, :], axis=0)\n",
    "        \n",
    "#         plt.plot(np.array(range(num_iterations)) * num_timesteps, mean, label=\"num_steps: {}\".format(num_timesteps))\n",
    "        plt.plot(np.array(range(num_iterations)), mean, label=\"num_steps: {}\".format(num_timesteps))\n",
    "        plt.fill_between(range(num_iterations), mean-std, mean+std, alpha=0.2)\n",
    "    plt.xlabel('number of expert trajectories', fontsize=16)\n",
    "    plt.ylabel(name, fontsize=16)\n",
    "    plt.legend()\n",
    "plt.savefig('expert_data_%s.png' % mode, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVeqRhMDgPqW"
   },
   "source": [
    "# You're Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW1 -- Blank.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
